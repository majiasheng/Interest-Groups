{"POST_ID":"43","GROUP_NAME":"math.statistics","GROUP_ID":"15","AUTHOR":"333","TIMESTAMP":"8/26/16 7:45 PM","SUBJECT":"Statistics and math","CONTENT":"There aren't many theorems in statistics that are likely to be of much interest to a pure mathematician. Many make use of math that's a century or more old, even if they weren't proven until relatively recently. Examples would include Cochran's theorem or the Central limit theorem, both of which are relatively young, dating only to the 1940s or so for rigorous proof, but are basically just variations on Fourier transforms, with some linear algebra and power series thrown in for good measure. The Frisch–Waugh–Lovell theorem is really just an application of linear projectors and the Eckart-Young theorem (Low-rank approximation) is just the SVD; the modern world would come to a screeching halt without it. So we're talking about cutting edge mathematics... in the 19th Century. (To be clear, I'm not saying this is bad work. It's not. It's very important.) Parts of statistics such as time series or information theory make use of deeper math, but even then I suspect most of the results wouldn't be that impressive to a pure mathematician. "}